,question,answer,contexts,ground_truth,context_precision,faithfulness,answer_relevancy,context_recall,context_relevancy,answer_correctness,answer_similarity
0,What are the two main tasks BERT is pre-trained on?,"BERT is pre-trained on the BooksCorpus and Wikipedia, totaling 800M and 2,500M words respectively. The pre-training tasks for BERT include Masked LM and the Masking Procedure.","['between how BERT and GPT were trained:\n• GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords).\n• GPT uses a sentence separator ( [SEP] ) and\nclassiﬁer token ( [CLS] ) which are only in-\ntroduced at ﬁne-tuning time; BERT learns\n[SEP] ,[CLS] and sentence A/Bembed-\ndings during pre-training.\n• GPT was trained for 1M steps with a batch\nsize of 32,000 words; BERT was trained for\n1M steps with a batch size of 128,000 words.\n• GPT used the same learning rate of 5e-5 for\nall ﬁne-tuning experiments; BERT chooses a\ntask-speciﬁc ﬁne-tuning learning rate which\nperforms the best on the development set.To isolate the effect of these differences, we per-\nform ablation experiments in Section 5.1 which\ndemonstrate that the majority of the improvements\nare in fact coming from the two pre-training tasks\nand the bidirectionality they enable.\nA.5 Illustrations of Fine-tuning on Different\nTasks\nThe illustration of ﬁne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speciﬁc\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks. In\nthe ﬁgure,Erepresents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classiﬁcation out-\nput, and [SEP] is the special symbol to separate'
 '2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to ﬁne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n3 BERT\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and ﬁne-tuning . Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture.\nModel Architecture BERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthetensor2tensor library.1Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to'
 'jority class.\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nforksteps. This allows us to answer the following\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200 400 600 800 1,0007678808284\nPre-training Steps (Thousands)MNLI Dev Accuracy\nBERT BASE (Masked LM)\nBERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for'
 'Le. 2018. QANet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. In ICLR .\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) .\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n19–27.\nAppendix for “BERT: Pre-training of\nDeep Bidirectional Transformers for\nLanguage Understanding”\nWe organize the appendix into three sections:\n• Additional implementation details for BERT\nare presented in Appendix A;• Additional details for our experiments are\npresented in Appendix B; and\n• Additional ablation studies are presented in\nAppendix C.\nWe present additional ablation studies for\nBERT including:\n–Effect of Number of Training Steps; and\n–Ablation for Different Masking Proce-\ndures.\nA Additional Details for BERT\nA.1 Illustration of the Pre-training Tasks\nWe provide examples of the pre-training tasks in\nthe following.\nMasked LM and the Masking Procedure As-\nsuming the unlabeled sentence is my dog is\nhairy , and during the random masking procedure\nwe chose the 4-th token (which corresponding to']",Masked LM (MLM) and Next Sentence Prediction (NSP).,0.999999999975,1.0,0.932955167090418,0.7142857142857143,0.0,0.8020059250786099,0.8080237003144394
1,"What model sizes are reported for BERT, and what are their specifications?","Two model sizes are reported for BERT: BERT BASE with specifications (L=12, H=768, A=12, Total Parameters=110M) and BERT LARGE with specifications (L=24, H=1024, A=16, Total Parameters=340M). BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Increasing model size leads to continual improvements on large-scale tasks such as machine translation.","['of Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as “The Annotated Transformer.”2\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERT BASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERT LARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERT BASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensorﬂow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\n4We note that in the literature the bidirectional Trans-'
 'on the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer.\n5.2 Effect of Model Size\nIn this section, we explore the effect of model size\non ﬁne-tuning task accuracy. We trained a number\nof BERT models with a differing number of layers,\nhidden units, and attention heads, while otherwise\nusing the same hyperparameters and training pro-\ncedure as described previously.\nResults on selected GLUE tasks are shown in\nTable 6. In this table, we report the average Dev\nSet accuracy from 5 random restarts of ﬁne-tuning.\nWe can see that larger models lead to a strict ac-\ncuracy improvement across all four datasets, even\nfor MRPC which only has 3,600 labeled train-\ning examples, and is substantially different from\nthe pre-training tasks. It is also perhaps surpris-\ning that we are able to achieve such signiﬁcant\nimprovements on top of models which are al-\nready quite large relative to the existing literature.\nFor example, the largest Transformer explored in\nVaswani et al. (2017) is (L=6, H=1024, A=16)\nwith 100M parameters for the encoder, and the\nlargest Transformer we have found in the literature\nis (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation'
 'best model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\nWe ﬁnd that BERT LARGE signiﬁcantly outper-\nforms BERT BASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2.\n4.2 SQuAD v1.1\nThe Stanford Question Answering Dataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016). Given a question and a passage from\n9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERT BASE and BERT LARGE .\n10https://gluebenchmark.com/leaderboardWikipedia containing the answer, the task is to\npredict the answer text span in the passage.']","BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).",0.9999999999666667,1.0,0.946293276502229,1.0,0.01020408163265306,0.6106008671546657,0.9424034686186626
2,How does BERT's architecture facilitate the use of a unified model across diverse NLP tasks?,"BERT's architecture facilitates the use of a unified model across diverse NLP tasks by pretraining deep bidirectional representations from unlabeled text, conditioning on both left and right context in all layers. This allows BERT to be fine-tuned with just one additional output layer for various tasks without substantial task-specific architecture modifications. BERT's bidirectional pre-training reduces the need for heavily-engineered task-specific architectures and achieves state-of-the-art performance on multiple NLP tasks.","['word based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer. In addi-\ntion to the masked language model, we also use\na “next sentence prediction” task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n• We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n• We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level andtoken-level tasks, outper-\nforming many task-speciﬁc architectures.\n• BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert .\n2 Related Work\nThere is a long history of pre-training general lan-\nguage representations, and we brieﬂy review the\nmost widely-used approaches in this section.'
 'BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\nAbstract\nWe introduce a new language representa-\ntion model called BERT , which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be ﬁne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeciﬁc architecture modiﬁcations.\nBERT is conceptually simple and empirically\npowerful. It obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n1 Introduction\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,']","BERT uses a multi-layer bidirectional Transformer encoder architecture, allowing for minimal task-specific architecture modifications in fine-tuning.",0.99999999995,1.0,0.932508827653234,1.0,0.02564102564102564,0.8698003183343449,0.9077727019088085
3,How does BERT's performance on the GLUE benchmark compare to previous state-of-the-art models?,"BERT BASE and BERT LARGE outperform all systems on the GLUE benchmark by a substantial margin, achieving 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. BERT LARGE significantly outperforms BERT BASE across all tasks, especially those with very little training data. BERT's performance on the GLUE benchmark is superior to previous state-of-the-art models.","['best model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\nWe ﬁnd that BERT LARGE signiﬁcantly outper-\nforms BERT BASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2.\n4.2 SQuAD v1.1\nThe Stanford Question Answering Dataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016). Given a question and a passage from\n9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERT BASE and BERT LARGE .\n10https://gluebenchmark.com/leaderboardWikipedia containing the answer, the task is to\npredict the answer text span in the passage.'
 'System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\nBERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\nWe use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERT LARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9']","BERT achieved new state-of-the-art on the GLUE benchmark (80.5%), surpassing the previous best models.",0.99999999995,1.0,0.9018396065771083,1.0,0.045454545454545456,0.4716622931545089,0.8866491726180358
4,"What significant improvements does BERT bring to the SQuAD v1.1,v2.0 and v13.5 tasks compared to prior models?","BERT brings significant improvements to the SQuAD v1.1, v2.0, and v13.5 tasks by outperforming all systems on all tasks by a substantial margin, obtaining average accuracy improvements over the prior state of the art. BERT LARGE significantly outperforms BERT BASE across all tasks, especially those with very little training data. The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.","['best model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\nWe ﬁnd that BERT LARGE signiﬁcantly outper-\nforms BERT BASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2.\n4.2 SQuAD v1.1\nThe Stanford Question Answering Dataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016). Given a question and a passage from\n9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERT BASE and BERT LARGE .\n10https://gluebenchmark.com/leaderboardWikipedia containing the answer, the task is to\npredict the answer text span in the passage.'
 'System Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman - - 82.3 91.2\n#1 Ensemble - nlnet - - 86.0 91.7\n#2 Ensemble - QANet - - 84.5 90.5\nPublished\nBiDAF+ELMo (Single) - 85.6 - 85.8\nR.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\nOurs\nBERT BASE (Single) 80.8 88.5 - -\nBERT LARGE (Single) 84.1 90.9 - -\nBERT LARGE (Ensemble) 85.8 91.8 - -\nBERT LARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\nBERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\nTable 2: SQuAD 1.1 results. The BERT ensemble\nis 7x systems which use different pre-training check-\npoints and ﬁne-tuning seeds.\nSystem Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman 86.3 89.0 86.9 89.5\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\n#2 Single - nlnet - - 74.2 77.1\nPublished\nunet (Ensemble) - - 71.4 74.9\nSLQA+ (Single) - 71.4 74.4\nOurs\nBERT LARGE (Single) 78.7 81.9 80.0 83.1\nTable 3: SQuAD 2.0 results. We exclude entries that\nuse BERT as one of their components.\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.12\n4.3 SQuAD v2.0\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem deﬁnition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\nWe use a simple approach to extend the SQuAD\nv1.1 BERT model for this task. We treat ques-\ntions that do not have an answer as having an an-\nswer span with start and end at the [CLS] to-\nken. The probability space for the start and end'
 '•Learning rate (Adam) : 5e-5, 3e-5, 2e-5\n•Number of epochs : 2, 3, 4\nWe also observed that large data sets (e.g.,\n100k+ labeled training examples) were far less\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is rea-\nsonable to simply run an exhaustive search over\nthe above parameters and choose the model that\nperforms best on the development set.\nA.4 Comparison of BERT, ELMo ,and\nOpenAI GPT\nHere we studies the differences in recent popular\nrepresentation learning models including ELMo,\nOpenAI GPT and BERT. The comparisons be-\ntween the model architectures are shown visually\nin Figure 3. Note that in addition to the architec-\nture differences, BERT and OpenAI GPT are ﬁne-\ntuning approaches, while ELMo is a feature-based\napproach.\nThe most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nleft-to-right Transformer LM on a large text cor-\npus. In fact, many of the design decisions in BERT\nwere intentionally made to make it as close to\nGPT as possible so that the two methods could be\nminimally compared. The core argument of this\nwork is that the bi-directionality and the two pre-\ntraining tasks presented in Section 3.1 account for\nthe majority of the empirical improvements, but\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:\n• GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords).']","BERT set new records on SQuAD v1.1 and v2.0, significantly outperforming the top leaderboard systems at the time. Version 13.5 doesn't exist. ",0.8333333332916666,1.0,0.9312944096201102,0.5,0.031746031746031744,0.7862308430621119,0.8949233722484473
5,Explain how BERT uses the 'masked LM' (MLM) for its pre-training.,BERT uses the 'masked LM' (MLM) for pre-training by masking some percentage of input tokens at random and predicting those masked tokens. This procedure allows BERT to train a deep bidirectional representation. The final hidden vectors corresponding to the mask tokens are fed into an output softmax.,"['segment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n3.1 Pre-training BERT\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right orright-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly “see itself”, and the model could trivially\npredict the target word in a multi-layered context.\nformer is often referred to as a “Transformer encoder” while\nthe left-context-only version is referred to as a “Transformer\ndecoder” since it can be used for text generation.In order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a “masked\nLM” (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the ﬁnal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over'
 'jority class.\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nforksteps. This allows us to answer the following\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200 400 600 800 1,0007678808284\nPre-training Steps (Thousands)MNLI Dev Accuracy\nBERT BASE (Masked LM)\nBERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for'
 'BERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps. The x-axis is the value of k.Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both ﬁne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npliﬁed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\nMasking Rates Dev Set Results\nMASK SAME RND MNLI NER\nFine-tune Fine-tune Feature-based\n80% 10% 10% 84.2 95.4 94.9\n100% 0% 0% 84.3 94.9 94.0\n80% 0% 20% 84.1 95.2 94.6\n80% 20% 0% 84.4 95.2 94.7\n0% 20% 80% 83.7 94.8 94.6\n0% 0% 100% 83.6 94.9 94.6\nTable 8: Ablation over different masking strategies.\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe[MASK] symbol for MLM; S AME means that\nwe keep the target token as is; R NDmeans that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speciﬁc strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the']","In MLM, a percentage of input tokens are masked randomly, and the model predicts these masked tokens based on their context.",0.9999999999666667,1.0,0.8678786611346592,1.0,0.0,0.8232869906004328,0.8931546012171874
6,Discuss the impact of model size on BERT's performance across different tasks.,"Increasing the model size of BERT leads to a strict accuracy improvement across various tasks, even with datasets that have limited training examples. Larger models show significant improvements over smaller ones, especially in tasks with minimal training data. BERT LARGE consistently outperforms BERT BASE across all tasks, showcasing the impact of model size on performance.","['on the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer.\n5.2 Effect of Model Size\nIn this section, we explore the effect of model size\non ﬁne-tuning task accuracy. We trained a number\nof BERT models with a differing number of layers,\nhidden units, and attention heads, while otherwise\nusing the same hyperparameters and training pro-\ncedure as described previously.\nResults on selected GLUE tasks are shown in\nTable 6. In this table, we report the average Dev\nSet accuracy from 5 random restarts of ﬁne-tuning.\nWe can see that larger models lead to a strict ac-\ncuracy improvement across all four datasets, even\nfor MRPC which only has 3,600 labeled train-\ning examples, and is substantially different from\nthe pre-training tasks. It is also perhaps surpris-\ning that we are able to achieve such signiﬁcant\nimprovements on top of models which are al-\nready quite large relative to the existing literature.\nFor example, the largest Transformer explored in\nVaswani et al. (2017) is (L=6, H=1024, A=16)\nwith 100M parameters for the encoder, and the\nlargest Transformer we have found in the literature\nis (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation'
 'best model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\nWe ﬁnd that BERT LARGE signiﬁcantly outper-\nforms BERT BASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2.\n4.2 SQuAD v1.1\nThe Stanford Question Answering Dataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016). Given a question and a passage from\n9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERT BASE and BERT LARGE .\n10https://gluebenchmark.com/leaderboardWikipedia containing the answer, the task is to\npredict the answer text span in the passage.']","Larger BERT models consistently show better performance across tasks, indicating the importance of model size in achieving high accuracy.",0.99999999995,1.0,0.9122914558725149,1.0,0.06756756756756757,0.879715241667431,0.9474323952411526
7,What datasets were used for BERT's pre-training and why?,"The datasets used for BERT's pre-training include unlabeled data over different pre-training tasks. The model is trained on these tasks before fine-tuning with labeled data from downstream tasks. BERT's architecture remains unified across different tasks, with minimal differences between pre-trained and downstream architectures.","['2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to ﬁne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n3 BERT\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and ﬁne-tuning . Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture.\nModel Architecture BERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthetensor2tensor library.1Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to'
 'Dev Set\nTasks MNLI-m QNLI MRPC SST-2 SQuAD\n(Acc) (Acc) (Acc) (Acc) (F1)\nBERT BASE 84.4 88.4 86.7 92.7 88.5\nNo NSP 83.9 84.9 86.5 92.6 87.9\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8\n+ BiLSTM 82.1 84.1 75.7 91.6 84.9\nTable 5: Ablation over the pre-training tasks using the\nBERT BASE architecture. “No NSP” is trained without\nthe next sentence prediction task. “LTR & No NSP” is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\ndomly initialized BiLSTM on top of the “LTR + No\nNSP” model during ﬁne-tuning.\nablation studies can be found in Appendix C.\n5.1 Effect of Pre-training Tasks\nWe demonstrate the importance of the deep bidi-\nrectionality of BERT by evaluating two pre-\ntraining objectives using exactly the same pre-\ntraining data, ﬁne-tuning scheme, and hyperpa-\nrameters as BERT BASE :\nNo NSP : A bidirectional model which is trained\nusing the “masked LM” (MLM) but without the\n“next sentence prediction” (NSP) task.\nLTR & No NSP : A left-context-only model which\nis trained using a standard Left-to-Right (LTR)\nLM, rather than an MLM. The left-only constraint\nwas also applied at ﬁne-tuning, because removing\nit introduced a pre-train/ﬁne-tune mismatch that\ndegraded downstream performance. Additionally,\nthis model was pre-trained without the NSP task.\nThis is directly comparable to OpenAI GPT, but\nusing our larger training dataset, our input repre-\nsentation, and our ﬁne-tuning scheme.\nWe ﬁrst examine the impact brought by the NSP']","BooksCorpus (800M words) and English Wikipedia (2500M words), chosen for their document-level organization and volume of data.",0.49999999995,1.0,0.9105206540071181,1.0,0.0,0.8368191540374467,0.7758480447212156
